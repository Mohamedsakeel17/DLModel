{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0foGY7EZ93S2"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a pre-trained BART model and tokenizer for summarization\n",
        "model_name = \"facebook/bart-large-cnn\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "def summarize_text(text, max_input_length=512, max_summary_length=100, min_summary_length=30, num_beams=4):\n",
        "    # Step 1: Pre-processing the text (Tokenization and truncation if too long)\n",
        "    inputs = tokenizer.encode(\"summarize: \" + text, return_tensors=\"pt\", max_length=max_input_length, truncation=True)\n",
        "\n",
        "    # Step 2: Generating the summary\n",
        "    summary_ids = model.generate(\n",
        "        inputs,\n",
        "        max_length=max_summary_length,\n",
        "        min_length=min_summary_length,\n",
        "        num_beams=num_beams, # Using beam search for more coherent summaries\n",
        "        length_penalty=2.0,  # Penalizing length for better output control\n",
        "        early_stopping=True\n",
        "    )\n",
        "\n",
        "    # Step 3: Decoding the output\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return summary\n",
        "\n",
        "# Input text to summarize\n",
        "text = \"\"\"\n",
        "    Artificial Intelligence (AI) is transforming the way we interact with technology. AI refers to systems or machines that\n",
        "    mimic human intelligence to perform tasks and can iteratively improve themselves based on the information they collect.\n",
        "    AI has been integrated into various fields including healthcare, finance, education, and more. This transformation is\n",
        "    driven by advancements in machine learning, a branch of AI, which enables systems to learn from data patterns and make\n",
        "    informed decisions. The implications of AI are vast, from automating routine tasks to enabling breakthroughs in areas\n",
        "    like drug discovery and climate change research.\n",
        "\"\"\"\n",
        "\n",
        "# Call the function to summarize the text\n",
        "summary = summarize_text(text)\n",
        "\n",
        "# Display the summary\n",
        "print(\"Summary:\", summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23j1y1iNAcXO",
        "outputId": "fc1060d2-61e5-4d41-ceaf-d00defb6fb7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary: Artificial Intelligence (AI) is transforming the way we interact with technology. The implications of AI are vast, from automating routine tasks to enabling breakthroughs in areas like drug discovery and climate change research.\n"
          ]
        }
      ]
    }
  ]
}